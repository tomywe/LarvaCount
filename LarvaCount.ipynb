{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "AvnD-ETlEttj",
    "outputId": "ad9844a9-0748-4547-bfed-72e07d161816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/anaconda3/lib/python3.7/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import datetime as dt\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from PIL import Image\n",
    "import wget\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision import models, transforms as T\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- change collection of images from videos to collect only one/two frames per second in order to produce a dataset that is less\\nlikely to have validation/test samples that are almost identical to the train set, such that it would create an ilusion of \\naccuracy over those sets while the real world generalization would be much wors that it appears to be.\\n\\n- expand dataset with new videos and randomly drop images to produce a more balanced dataset so as to avoid seamingly higher \\naccuracy than actually exists due to data bias towards count numbers that are better represented.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- change collection of images from videos to collect only one/two frames per second in order to produce a dataset that is less\n",
    "likely to have validation/test samples that are almost identical to the train set, such that it would create an ilusion of \n",
    "accuracy over those sets while the real world generalization would be much wors that it appears to be.\n",
    "\n",
    "- expand dataset with new videos and randomly drop images to produce a more balanced dataset so as to avoid seamingly higher \n",
    "accuracy than actually exists due to data bias towards count numbers that are better represented.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ThEZ2VZrjvaQ",
    "outputId": "d59822d4-ffea-41b1-ef43-7e0a1a9f1115"
   },
   "outputs": [],
   "source": [
    "LarvaData = pd.read_csv('larva_count.csv')\n",
    "LarvaData.dropna(inplace=True)\n",
    "len(LarvaData)\n",
    "LarvaData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1DdhrOgVhyV5",
    "outputId": "41e4bba5-00fa-419c-c201-3d2834bec6dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘videos’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir videos\n",
    "\n",
    "if 'videos' not in os.listdir():\n",
    "    min_count = min(LarvaData['Manual count'])\n",
    "    max_count = max(LarvaData['Manual count'])\n",
    "    count_counts = np.zeros(int(max_count - min_count + 1))\n",
    "    N_data = len(LarvaData)\n",
    "    for i, (link, count) in enumerate(zip(LarvaData.Link, LarvaData['Manual count'])):\n",
    "        clear_output(wait=True)\n",
    "        print('Current progress:', np.round(i/ N_data * 100, 2), '%')\n",
    "        print(link)\n",
    "        v_name = link.split('/')[-1]\n",
    "        wget.download(link, out='videos')\n",
    "        cap = cv2.VideoCapture('videos/' + v_name)\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        count_counts[int(count-min_count)] += frameCount\n",
    "    mod_counts = np.around(count_counts / 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xlLOQisJqFwC",
    "outputId": "876d4dd6-3fbc-439a-8574-d8ce3abdd5fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘images’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir images\n",
    "\n",
    "if 'images' not in os.listdir():\n",
    "    LarvaData = LarvaData.dropna()\n",
    "    num_images = 0\n",
    "    im_data = pd.DataFrame(columns=['names', 'counts'])\n",
    "    num_videos = 0\n",
    "    for link, count in zip(LarvaData.Link, LarvaData['Manual count']):\n",
    "        #wget.download(link, out='videos')\n",
    "        num_videos += 1\n",
    "        clear_output(wait=True)\n",
    "        print('Current progress:', np.round(num_videos/ N_data * 100, 2), '%')\n",
    "        v_name = link.split('/')[-1]\n",
    "        cap = cv2.VideoCapture('videos/' + v_name)\n",
    "        fc = 0\n",
    "        ret = True\n",
    "        mod_count = int(mod_counts[int(count - min_count)])\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if mod_count == 0:\n",
    "            mod_count = 1\n",
    "        while (fc < frameCount and ret):\n",
    "            ret, im = cap.read()\n",
    "            fc += 1\n",
    "            if fc%mod_count == 0 and ret:\n",
    "                im_name = v_name.split('.')[0] + '_' + str(fc) + '.jpg'\n",
    "                im_path = 'images/' + im_name\n",
    "                cv2.imwrite(im_path, im)\n",
    "                im_data = im_data.append({'names': im_name, 'counts': count}, ignore_index=True)\n",
    "                num_images += 1\n",
    "                if num_images%500 == 0:\n",
    "                    print(num_images)\n",
    "    print('Images dataset size is', num_images)\n",
    "    print(dt.datetime.now())\n",
    "    im_data.to_csv('im_data.csv', index=False)\n",
    "    os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnoJZ5qTbB9g"
   },
   "outputs": [],
   "source": [
    "im_data = pd.read_csv('im_data.csv', usecols=['names', 'counts'])\n",
    "im_data = im_data.dropna()\n",
    "im_data.index = list(range(len(im_data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v7KmKpwDDcv6",
    "outputId": "9e41b4a1-8828-4e88-fae1-37ca49b50c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)\n",
    "  \n",
    "class LarvaDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.names = dataset.names\n",
    "        self.counts = dataset.counts\n",
    "        self.transform = transform\n",
    "        self.len = len(dataset)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        count = torch.tensor(self.counts[index], device=device)\n",
    "        im = Image.open('images/' + self.names[index])\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "            im.to(device)\n",
    "        return im, count\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "  \n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "transform_train = T.Compose([T.Resize(255, interpolation=Image.BICUBIC), \n",
    "                             T.ColorJitter(.1, .1, .1, .1), \n",
    "                             T.RandomAffine(0, translate=(0.1, 0.1)), \n",
    "                             T.ToTensor(), \n",
    "                             T.Normalize(mean, std)])\n",
    "transform_eval = T.Compose([T.Resize(255, interpolation=Image.BICUBIC), \n",
    "                            T.ToTensor(), \n",
    "                            T.Normalize(mean, std)])\n",
    "\n",
    "\n",
    "train_names, test_names, train_counts, test_counts = train_test_split(im_data.names, im_data.counts, test_size=0.2)\n",
    "train_names, val_names, train_counts, val_counts = train_test_split(train_names, train_counts, test_size=0.25)\n",
    "train_df = pd.DataFrame(data=np.array([train_names.values, train_counts.values]).T, columns=['names', 'counts'], \n",
    "                        index=list(range(len(train_counts))))\n",
    "val_df = pd.DataFrame(data=np.array([val_names.values, val_counts.values]).T, columns=['names', 'counts'], \n",
    "                      index=list(range(len(val_counts))))\n",
    "test_df = pd.DataFrame(data=np.array([test_names.values, test_counts.values]).T, columns=['names', 'counts'], \n",
    "                       index=list(range(len(test_counts))))\n",
    "\n",
    "train_set = LarvaDataset(train_df, transform=transform_train)\n",
    "val_set = LarvaDataset(val_df, transform=transform_eval)\n",
    "test_set = LarvaDataset(test_df, transform=transform_eval)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = 64, sampler=SubsetRandomSampler(range(len(train_set))))\n",
    "val_loader = DataLoader(val_set, batch_size = 128, sampler=SubsetRandomSampler(range(len(val_set))))\n",
    "test_loader = DataLoader(test_set, batch_size = 128, sampler=SubsetRandomSampler(range(len(test_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNycOu8Gqj1c"
   },
   "outputs": [],
   "source": [
    "# defining the training procedure\n",
    "\n",
    "def train(model, optimizer, loader, val_loader, PATH, epochs=50, schedualer=None, best_v_loss=None):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    if best_v_loss is None:\n",
    "        best_v_loss = (im_data['counts'].max())**2\n",
    "    for e in range(epochs):\n",
    "        print()\n",
    "        print('current learning rate is lr = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        print(dt.datetime.now())\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        for t, (d, l) in enumerate(loader):\n",
    "            model.train()\n",
    "            model.to(device)\n",
    "            data = d.to(device)\n",
    "            labels = l.to(device)\n",
    "            scores, _ = model(data)\n",
    "            loss = F.mse_loss(scores.view(-1), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            print('ephoch {0} iteration {1} train loss {2}'.format(e, t, loss))\n",
    "            torch.cuda.empty_cache()\n",
    "            if t%250 == 0:\n",
    "                print('ephoch {0} iteration {1} train loss {2}'.format(e, t, loss))\n",
    "                print(dt.datetime.now())\n",
    "                val_loss.append(accuracy(model, val_loader))\n",
    "                print(dt.datetime.now())\n",
    "                train_loss.append(loss)\n",
    "                print('GPU memory allocated {}'.format(torch.cuda.memory_allocated()))\n",
    "                print()\n",
    "                if val_loss[-1] < best_v_loss:\n",
    "                    best_v_loss = val_loss[-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save({\n",
    "                            'epoch': e,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': loss,\n",
    "                            'val_loss':val_loss[-1],\n",
    "                            }, PATH)\n",
    "        if schedualer is not None:\n",
    "            schedualer.step()\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "# deting the accuracy of model preductions of a dataset\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    loss_list = []\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    running_mean = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            model.to(device)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            scores, _ = model(data)\n",
    "            preds = scores.view(-1)\n",
    "            loss_list.append(F.mse_loss(preds, labels))\n",
    "            num_correct += ((preds - labels).abs()/labels <= 0.05).sum()\n",
    "            num_samples += len(labels)\n",
    "            current_mean = ((preds - labels).abs()/labels).mean()\n",
    "            running_mean += ((preds - labels).abs()/labels).sum()\n",
    "            torch.cuda.empty_cache()\n",
    "        running_mean /= num_samples\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('got {0} / {1} correct at {2}%'.format(num_correct, num_samples, acc*100))\n",
    "        print('the average accuracy is {}%'.format(running_mean*100))\n",
    "        torch.cuda.empty_cache()\n",
    "        return np.mean(torch.tensor(loss_list).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_n46oyj3Um3v"
   },
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512, 256, 128, 64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained=True)\n",
    "            self._initialize_weights()\n",
    "            for i in xrange(len(self.frontend.state_dict().items())):\n",
    "                self.frontend.state_dict().items()[i][1].data[:] = mod.state_dict().items()[i][1].data[:]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        s = x.sum((1, 2, 3))\n",
    "        return s, x\n",
    "      \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "                \n",
    "def make_layers(cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFpyc7kynxBM"
   },
   "outputs": [],
   "source": [
    "csrnet = CSRNet(load_weights=True)\n",
    "optimizer = optim.Adam(csrnet.parameters(), lr=1e-6)\n",
    "\n",
    "CP_PATH = 'checkpoint.pth.tar'\n",
    "try:\n",
    "    checkpoint = torch.load(CP_PATH)\n",
    "    best_v_loss = checkpoint['val_loss']\n",
    "    csrnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "except:\n",
    "    checkpoint = torch.load('0model_best.pth.tar')\n",
    "    csrnet.load_state_dict(checkpoint['state_dict'])\n",
    "    best_v_loss = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1j7dQ566ikDM"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(csrnet.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Baqg0sQYuxyG"
   },
   "outputs": [],
   "source": [
    "csrnet.to(device)\n",
    "schedualer = optim.lr_scheduler.StepLR(optimizer, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OYDQO0wvlDYa",
    "outputId": "32f00419-b6bf-4055-f51a-e321d4da4d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current learning rate is lr = 1e-07\n",
      "2019-08-07 18:43:17.583741\n",
      "\n",
      "ephoch 0 iteration 0 train loss 105.72002410888672\n",
      "ephoch 0 iteration 0 train loss 105.72002410888672\n",
      "2019-08-07 18:43:25.903778\n",
      "got 13399 / 15881 correct at 84.37126125558844%\n",
      "the average accuracy is 2.790194272994995%\n",
      "2019-08-07 19:05:12.057875\n",
      "GPU memory allocated 623521792\n",
      "\n",
      "ephoch 0 iteration 1 train loss 98.63580322265625\n",
      "ephoch 0 iteration 2 train loss 73.87606811523438\n",
      "ephoch 0 iteration 3 train loss 119.40758514404297\n",
      "ephoch 0 iteration 4 train loss 94.2315673828125\n",
      "ephoch 0 iteration 5 train loss 104.3741455078125\n",
      "ephoch 0 iteration 6 train loss 131.30043029785156\n",
      "ephoch 0 iteration 7 train loss 83.12932586669922\n",
      "ephoch 0 iteration 8 train loss 76.46619415283203\n",
      "ephoch 0 iteration 9 train loss 77.45465087890625\n",
      "ephoch 0 iteration 10 train loss 103.29241943359375\n",
      "ephoch 0 iteration 11 train loss 66.63739776611328\n",
      "ephoch 0 iteration 12 train loss 98.79231262207031\n",
      "ephoch 0 iteration 13 train loss 73.57967376708984\n",
      "ephoch 0 iteration 14 train loss 121.6446762084961\n",
      "ephoch 0 iteration 15 train loss 92.43721008300781\n",
      "ephoch 0 iteration 16 train loss 68.48703002929688\n",
      "ephoch 0 iteration 17 train loss 103.43441772460938\n",
      "ephoch 0 iteration 18 train loss 88.61132049560547\n",
      "ephoch 0 iteration 19 train loss 85.19876098632812\n",
      "ephoch 0 iteration 20 train loss 87.41244506835938\n",
      "ephoch 0 iteration 21 train loss 75.39537048339844\n",
      "ephoch 0 iteration 22 train loss 59.21388244628906\n",
      "ephoch 0 iteration 23 train loss 98.22467041015625\n",
      "ephoch 0 iteration 24 train loss 92.16468048095703\n",
      "ephoch 0 iteration 25 train loss 101.67141723632812\n",
      "ephoch 0 iteration 26 train loss 64.31905364990234\n",
      "ephoch 0 iteration 27 train loss 90.29332733154297\n",
      "ephoch 0 iteration 28 train loss 92.91667175292969\n",
      "ephoch 0 iteration 29 train loss 98.65653991699219\n",
      "ephoch 0 iteration 30 train loss 80.08189392089844\n",
      "ephoch 0 iteration 31 train loss 170.1326141357422\n",
      "ephoch 0 iteration 32 train loss 75.990478515625\n",
      "ephoch 0 iteration 33 train loss 152.80496215820312\n",
      "ephoch 0 iteration 34 train loss 87.232177734375\n",
      "ephoch 0 iteration 35 train loss 58.77202224731445\n",
      "ephoch 0 iteration 36 train loss 84.18310546875\n",
      "ephoch 0 iteration 37 train loss 103.72410583496094\n",
      "ephoch 0 iteration 38 train loss 124.00350952148438\n",
      "ephoch 0 iteration 39 train loss 94.95707702636719\n",
      "ephoch 0 iteration 40 train loss 76.736328125\n",
      "ephoch 0 iteration 41 train loss 160.19984436035156\n",
      "ephoch 0 iteration 42 train loss 114.05255126953125\n",
      "ephoch 0 iteration 43 train loss 114.6239013671875\n",
      "ephoch 0 iteration 44 train loss 119.54462432861328\n",
      "ephoch 0 iteration 45 train loss 184.536865234375\n",
      "ephoch 0 iteration 46 train loss 111.0201416015625\n",
      "ephoch 0 iteration 47 train loss 120.56756591796875\n",
      "ephoch 0 iteration 48 train loss 65.50824737548828\n",
      "ephoch 0 iteration 49 train loss 83.68413543701172\n",
      "ephoch 0 iteration 50 train loss 131.1997833251953\n",
      "ephoch 0 iteration 51 train loss 80.96794891357422\n",
      "ephoch 0 iteration 52 train loss 88.62813568115234\n",
      "ephoch 0 iteration 53 train loss 65.61393737792969\n",
      "ephoch 0 iteration 54 train loss 114.91521453857422\n",
      "ephoch 0 iteration 55 train loss 145.03811645507812\n",
      "ephoch 0 iteration 56 train loss 95.92221069335938\n",
      "ephoch 0 iteration 57 train loss 112.20794677734375\n",
      "ephoch 0 iteration 58 train loss 92.6662368774414\n",
      "ephoch 0 iteration 59 train loss 97.08259582519531\n",
      "ephoch 0 iteration 60 train loss 148.8020782470703\n",
      "ephoch 0 iteration 61 train loss 70.27984619140625\n",
      "ephoch 0 iteration 62 train loss 81.66181182861328\n",
      "ephoch 0 iteration 63 train loss 77.27899169921875\n",
      "ephoch 0 iteration 64 train loss 95.45879364013672\n",
      "ephoch 0 iteration 65 train loss 127.83751678466797\n",
      "ephoch 0 iteration 66 train loss 100.44950866699219\n",
      "ephoch 0 iteration 67 train loss 93.33558654785156\n",
      "ephoch 0 iteration 68 train loss 91.76521301269531\n",
      "ephoch 0 iteration 69 train loss 86.99588012695312\n",
      "ephoch 0 iteration 70 train loss 63.466976165771484\n",
      "ephoch 0 iteration 71 train loss 177.56832885742188\n",
      "ephoch 0 iteration 86 train loss 94.35444641113281\n",
      "ephoch 0 iteration 87 train loss 83.72105407714844\n",
      "ephoch 0 iteration 88 train loss 170.57241821289062\n",
      "ephoch 0 iteration 89 train loss 81.12294006347656\n",
      "ephoch 0 iteration 90 train loss 107.990478515625\n",
      "ephoch 0 iteration 91 train loss 88.47186279296875\n",
      "ephoch 0 iteration 92 train loss 94.73806762695312\n",
      "ephoch 0 iteration 93 train loss 66.72813415527344\n",
      "ephoch 0 iteration 94 train loss 83.54891967773438\n",
      "ephoch 0 iteration 95 train loss 107.1649169921875\n",
      "ephoch 0 iteration 96 train loss 71.2885513305664\n",
      "ephoch 0 iteration 97 train loss 85.27015686035156\n",
      "ephoch 0 iteration 98 train loss 110.47798156738281\n",
      "ephoch 0 iteration 99 train loss 65.03726196289062\n",
      "ephoch 0 iteration 100 train loss 82.10459899902344\n",
      "ephoch 0 iteration 101 train loss 79.15921020507812\n",
      "ephoch 0 iteration 102 train loss 82.42277526855469\n",
      "ephoch 0 iteration 103 train loss 90.84463500976562\n",
      "ephoch 0 iteration 104 train loss 99.39354705810547\n",
      "ephoch 0 iteration 105 train loss 72.12361907958984\n",
      "ephoch 0 iteration 106 train loss 91.49569702148438\n",
      "ephoch 0 iteration 107 train loss 72.14131927490234\n",
      "ephoch 0 iteration 108 train loss 103.96565246582031\n",
      "ephoch 0 iteration 109 train loss 93.012451171875\n",
      "ephoch 0 iteration 110 train loss 83.91350555419922\n",
      "ephoch 0 iteration 111 train loss 69.27825164794922\n",
      "ephoch 0 iteration 112 train loss 76.53701782226562\n",
      "ephoch 0 iteration 113 train loss 64.80656433105469\n",
      "ephoch 0 iteration 114 train loss 131.97964477539062\n",
      "ephoch 0 iteration 115 train loss 97.33711242675781\n",
      "ephoch 0 iteration 116 train loss 144.06228637695312\n",
      "ephoch 0 iteration 117 train loss 87.46063995361328\n",
      "ephoch 0 iteration 118 train loss 94.04570007324219\n",
      "ephoch 0 iteration 119 train loss 73.9532470703125\n",
      "ephoch 0 iteration 120 train loss 65.16827392578125\n",
      "ephoch 0 iteration 121 train loss 94.59457397460938\n",
      "ephoch 0 iteration 122 train loss 61.03577423095703\n",
      "ephoch 0 iteration 123 train loss 85.838134765625\n",
      "ephoch 0 iteration 124 train loss 89.91398620605469\n",
      "ephoch 0 iteration 125 train loss 114.57888793945312\n",
      "ephoch 0 iteration 126 train loss 95.12557220458984\n",
      "ephoch 0 iteration 127 train loss 71.60086059570312\n",
      "ephoch 0 iteration 128 train loss 102.87614440917969\n",
      "ephoch 0 iteration 129 train loss 108.96363830566406\n",
      "ephoch 0 iteration 130 train loss 132.65335083007812\n",
      "ephoch 0 iteration 131 train loss 127.8267822265625\n",
      "ephoch 0 iteration 132 train loss 61.30853271484375\n",
      "ephoch 0 iteration 133 train loss 116.87165832519531\n",
      "ephoch 0 iteration 134 train loss 67.2687759399414\n",
      "ephoch 0 iteration 135 train loss 82.37094116210938\n",
      "ephoch 0 iteration 136 train loss 114.19685363769531\n",
      "ephoch 0 iteration 137 train loss 68.44048309326172\n",
      "ephoch 0 iteration 138 train loss 86.34805297851562\n",
      "ephoch 0 iteration 139 train loss 69.1109848022461\n",
      "ephoch 0 iteration 140 train loss 91.40322875976562\n",
      "ephoch 0 iteration 141 train loss 121.4907455444336\n",
      "ephoch 0 iteration 142 train loss 97.49015808105469\n",
      "ephoch 0 iteration 143 train loss 100.10369873046875\n",
      "ephoch 0 iteration 144 train loss 76.82533264160156\n",
      "ephoch 0 iteration 145 train loss 88.65115356445312\n",
      "ephoch 0 iteration 146 train loss 95.56922149658203\n",
      "ephoch 0 iteration 147 train loss 67.26718139648438\n",
      "ephoch 0 iteration 148 train loss 71.88703155517578\n",
      "ephoch 0 iteration 149 train loss 92.33491516113281\n",
      "ephoch 0 iteration 150 train loss 104.02024841308594\n",
      "ephoch 0 iteration 151 train loss 114.55422973632812\n",
      "ephoch 0 iteration 152 train loss 72.25220489501953\n",
      "ephoch 0 iteration 153 train loss 88.62886047363281\n",
      "ephoch 0 iteration 154 train loss 102.6895751953125\n",
      "ephoch 0 iteration 155 train loss 86.2940673828125\n",
      "ephoch 0 iteration 156 train loss 87.57955932617188\n",
      "ephoch 0 iteration 157 train loss 76.75819396972656\n",
      "ephoch 0 iteration 158 train loss 105.54434204101562\n",
      "ephoch 0 iteration 159 train loss 90.03736877441406\n",
      "ephoch 0 iteration 160 train loss 95.57682800292969\n",
      "ephoch 0 iteration 161 train loss 195.35574340820312\n",
      "ephoch 0 iteration 162 train loss 81.8265609741211\n",
      "ephoch 0 iteration 163 train loss 83.23289489746094\n",
      "ephoch 0 iteration 164 train loss 95.7890853881836\n",
      "ephoch 0 iteration 165 train loss 170.86138916015625\n",
      "ephoch 0 iteration 166 train loss 71.05746459960938\n",
      "ephoch 0 iteration 167 train loss 76.3782958984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephoch 0 iteration 168 train loss 150.60076904296875\n",
      "ephoch 0 iteration 169 train loss 65.7871322631836\n",
      "ephoch 0 iteration 170 train loss 110.51423645019531\n",
      "ephoch 0 iteration 171 train loss 82.80037689208984\n",
      "ephoch 0 iteration 172 train loss 101.88916015625\n",
      "ephoch 0 iteration 173 train loss 118.10134887695312\n",
      "ephoch 0 iteration 174 train loss 133.06784057617188\n",
      "ephoch 0 iteration 175 train loss 128.94210815429688\n",
      "ephoch 0 iteration 176 train loss 106.85215759277344\n",
      "ephoch 0 iteration 177 train loss 102.62232971191406\n",
      "ephoch 0 iteration 178 train loss 81.36483764648438\n",
      "ephoch 0 iteration 179 train loss 120.67059326171875\n",
      "ephoch 0 iteration 180 train loss 76.14460754394531\n",
      "ephoch 0 iteration 181 train loss 85.79192352294922\n",
      "ephoch 0 iteration 182 train loss 85.15878295898438\n",
      "ephoch 0 iteration 183 train loss 77.25363159179688\n",
      "ephoch 0 iteration 184 train loss 80.76475524902344\n",
      "ephoch 0 iteration 185 train loss 106.25869750976562\n",
      "ephoch 0 iteration 186 train loss 100.01734924316406\n",
      "ephoch 0 iteration 187 train loss 104.25179290771484\n",
      "ephoch 0 iteration 188 train loss 106.37223815917969\n",
      "ephoch 0 iteration 189 train loss 164.5697021484375\n",
      "ephoch 0 iteration 190 train loss 71.08770751953125\n",
      "ephoch 0 iteration 191 train loss 97.13905334472656\n",
      "ephoch 0 iteration 192 train loss 78.93833923339844\n",
      "ephoch 0 iteration 193 train loss 88.05119323730469\n",
      "ephoch 0 iteration 194 train loss 117.57615661621094\n",
      "ephoch 0 iteration 195 train loss 87.33139038085938\n",
      "ephoch 0 iteration 196 train loss 114.41669464111328\n",
      "ephoch 0 iteration 197 train loss 132.3874969482422\n",
      "ephoch 0 iteration 198 train loss 86.51534271240234\n",
      "ephoch 0 iteration 199 train loss 114.39688110351562\n",
      "ephoch 0 iteration 200 train loss 74.77848052978516\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(csrnet, optimizer, train_loader, val_loader, CP_PATH, epochs=10, best_v_loss=best_v_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7O4LMyjXsQxb"
   },
   "outputs": [],
   "source": [
    "def count(path, model):\n",
    "    \"\"\"\n",
    "    evaluates the number of larva present in input.\n",
    "    input is either an image of a video. if input is an image, the evaluation is done once over the image, if input is a\n",
    "    video, the evaluation is done over every caption in the video seperately and then averaged over all captions to\n",
    "    produce the result\n",
    "    :param path: a path to an image or a video\n",
    "    :return: count\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define the device(processor) type\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    # Load the image or video\n",
    "    im_list = []\n",
    "    try:\n",
    "        im_list.append(Image.open(path))\n",
    "    except OSError:\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fc = 0\n",
    "        ret = True\n",
    "        im_list = []\n",
    "        while (fc < frameCount and ret):\n",
    "            ret, im = cap.read()\n",
    "            if fc%10 == 0:\n",
    "                #new_im = np.zeros_like(im)\n",
    "                #new_im[:,:,0] = im[:,:,2]\n",
    "                #new_im[:,:,1] = im[:,:,1]\n",
    "                #new_im[:,:,2] = im[:,:,0]\n",
    "                im_list.append(Image.fromarray(im.astype('uint8')))\n",
    "            fc += 1\n",
    "\n",
    "    # Disable gradients\n",
    "    with torch.no_grad():\n",
    "        # Prepare data for model\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        transform_eval = T.Compose([T.Resize(255, interpolation=Image.BICUBIC), T.ToTensor(), T.Normalize(mean, std)])\n",
    "        model_input = torch.stack([torch.tensor(transform_eval(im), device=device) for im in im_list])\n",
    "        model_input.to(device)\n",
    "        results, densities = model(model_input)\n",
    "        if len(results) > 1:\n",
    "            results_mean = results.mean()\n",
    "        return results_mean, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COzxpMszi01R"
   },
   "outputs": [],
   "source": [
    "video_names = os.listdir('videos')\n",
    "results = []\n",
    "labels = []\n",
    "errors = []\n",
    "for i, v_name in enumerate(video_names):\n",
    "    r = count('videos/' + v_name, csrnet)\n",
    "    v_idx = np.where(im_data.names == (v_name.split('.')[0]+'_1.jpg'))[0][0]\n",
    "    l = im_data.counts[v_idx]\n",
    "    results.append(r)\n",
    "    labels.append(l)\n",
    "    errors.append((r-l)/l)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4JELMYXi45C"
   },
   "outputs": [],
   "source": [
    "results = torch.tensor(results)\n",
    "labels = torch.tensor(labels)\n",
    "errors = torch.tensor(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJPV37XDi9ol"
   },
   "outputs": [],
   "source": [
    "plt.scatter(err.numpy(), labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rx4VqpYqjAA_"
   },
   "outputs": [],
   "source": [
    "for n in im_data.names:\n",
    "    print(n.split('_')[1].split('.')[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LarvaCount.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
